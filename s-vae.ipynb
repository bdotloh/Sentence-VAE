{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SentenceVAE\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize data \n",
    "def preprocess_and_tokenise(data):\n",
    "    sk_word_tokenize = cv.build_tokenizer()\n",
    "    sk_preprocesser = cv.build_preprocessor()\n",
    "    tokenize = lambda doc: sk_word_tokenize(sk_preprocesser(doc))\n",
    "    data_tokenised=[tokenize(dat) for dat in data]\n",
    "    return data_tokenised\n",
    "\n",
    "\n",
    "train_tokenised=preprocess_and_tokenise(train)\n",
    "test_tokenised=preprocess_and_tokenise(test)\n",
    "valid_tokenised=preprocess_and_tokenise(valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocablurary of 8293 keys created.\n"
     ]
    }
   ],
   "source": [
    "# create vocab from training data\n",
    "cv = sklearn.feature_extraction.text.CountVectorizer(lowercase=True).fit(train)\n",
    "\n",
    "vocab=dict(sorted(cv.vocabulary_.items(),key=lambda x: x[1]))\n",
    "# +4 to all indexs to make way for special tokens\n",
    "vocab={k:v+4 for k,v in vocab.items()}\n",
    "\n",
    "# add special tokens\n",
    "special_tokens = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "for i,token in enumerate(special_tokens):\n",
    "    vocab[token]=i\n",
    "\n",
    "w2i=dict(sorted(vocab.items(),key=lambda x: x[1]))\n",
    "i2w={i:w for w,i in w2i.items()}\n",
    "\n",
    "print(\"Vocablurary of %i keys created.\" %len(w2i))\n",
    "\n",
    "# store w2i and i2w in dict\n",
    "vocab=dict(w2i=w2i, i2w=i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to prepare inputs\n",
    "from collections import defaultdict\n",
    "\n",
    "def prepare_data(data,max_sequence_length=50):\n",
    "    data_processed = defaultdict(dict)\n",
    "\n",
    "    inputs=[['<sos>']+tokens for tokens in data]\n",
    "    inputs=[tokens[:max_sequence_length] for tokens in inputs]\n",
    "\n",
    "    targets=[tokens[:max_sequence_length-1] for tokens in data]\n",
    "    targets=[tokens+['<eos>'] for tokens in targets]\n",
    "\n",
    "    lengths=[len(tokens) for tokens in inputs]\n",
    "\n",
    "    [inputs[i].extend(['<pad>'] * (max_sequence_length-lengths[i])) for i in range(len(inputs))]\n",
    "    [targets[i].extend(['<pad>'] * (max_sequence_length-lengths[i])) for i in range(len(targets))]\n",
    "\n",
    "    inputs=[[w2i.get(w,w2i['<unk>']) for w in tokens] for tokens in inputs]\n",
    "    targets=[[w2i.get(w,w2i['<unk>']) for w in tokens] for tokens in targets]\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        data_processed[i]['input']=inputs[i]\n",
    "        data_processed[i]['target']=targets[i]\n",
    "        data_processed[i]['length']=lengths[i]\n",
    "\n",
    "    return data_processed\n",
    "\n",
    "    \n",
    "train_processed= prepare_data(train_tokenised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "es_conv=pd.read_json('data/ESConv.json')\n",
    "es_conv['supporter_utterances']=es_conv.dialog.apply(lambda x : process_dialogue(x)['supporter'])\n",
    "\n",
    "supporter_utts=[utt for utts in es_conv.supporter_utterances.values.tolist() for utt in utts]\n",
    "supporter_utts\n",
    "train,test=train_test_split(supporter_utts,test_size=.15, random_state=42)\n",
    "train,valid=train_test_split(supporter_utts, test_size=.15, random_state=42)\n",
    "\n",
    "print(\"Train:\", len(train))\n",
    "print(\"Test:\", len(test))\n",
    "print(\"Validation:\", len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 13026\n",
      "Test: 2299\n",
      "Validation: 2299\n",
      "Train: 13026\n",
      "Test: 2299\n",
      "Validation: 2299\n",
      "Train: 13026\n",
      "Test: 2299\n",
      "Validation: 2299\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import sklearn.feature_extraction.text \n",
    "import sklearn.preprocessing\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ESCONV(Dataset):\n",
    "    def __init__(self, data_dir='data/ESConv.json', load_split='train',  max_sequence_length=50):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_sequence_length=max_sequence_length\n",
    "        self.es_conv=pd.read_json('data/ESConv.json')\n",
    "       \n",
    "        # i am only interested in supporter utterances for now\n",
    "        self.es_conv['supporter_utterances']=es_conv.dialog.apply(lambda x : self._retrieve_utterances(x)['supporter'])\n",
    "        supporter_utts=[utt for utts in es_conv.supporter_utterances.values.tolist() for utt in utts]\n",
    "\n",
    "        train,test=train_test_split(supporter_utts,test_size=.15, random_state=42)\n",
    "        train,valid=train_test_split(supporter_utts, test_size=.15, random_state=42)\n",
    "\n",
    "        print(\"Train:\", len(train))\n",
    "        print(\"Test:\", len(test))\n",
    "        print(\"Validation:\", len(valid))\n",
    "\n",
    "        # fit cv on train\n",
    "        self.cv = sklearn.feature_extraction.text.CountVectorizer(lowercase=True)\n",
    "        self.cv.fit(train)\n",
    "\n",
    "        self.splits={\n",
    "            'train':self._preprocess_and_tokenise_data(train),\n",
    "            'valid': self._preprocess_and_tokenise_data(valid),\n",
    "            'test':self._preprocess_and_tokenise_data(test)\n",
    "            }\n",
    "\n",
    "\n",
    "        #initialise vocab\n",
    "        self._create_vocab()\n",
    "\n",
    "        self.data=self._prepare_data(split=load_split)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input': np.asarray(self.data[idx]['input']),\n",
    "            'target': np.asarray(self.data[idx]['target']),\n",
    "            'length': self.data[idx]['length']\n",
    "        }\n",
    "\n",
    "    def _preprocess_and_tokenise_data(self,data):\n",
    "        sk_word_tokenize = self.cv.build_tokenizer()\n",
    "        sk_preprocesser = self.cv.build_preprocessor()\n",
    "        tokenize = lambda doc: sk_word_tokenize(sk_preprocesser(doc))\n",
    "        data_tokenised=[tokenize(dat) for dat in data]\n",
    "        \n",
    "        return data_tokenised\n",
    "\n",
    "    def _retrieve_utterances(self, dialog):\n",
    "        \"\"\"\n",
    "        takes a dialog, returns a speaker:[utt list] dict \n",
    "        \"\"\"\n",
    "\n",
    "        prev_speaker=None\n",
    "        curr_speaker=None\n",
    "    \n",
    "        all_utterances=dict(zip(['seeker','supporter'],[[] for _ in range(2)]))\n",
    "\n",
    "        for i,item in enumerate(dialog):\n",
    "            prev_speaker=curr_speaker\n",
    "\n",
    "            curr_speaker=item['speaker'].strip()\n",
    "            curr_utt=item['content'].strip()\n",
    "            \n",
    "            if curr_speaker==prev_speaker:  # concat curr utterance to previous utterance\"\n",
    "                all_utterances[curr_speaker][-1]=f\"{all_utterances[curr_speaker][-1]}. {curr_utt}\"\n",
    "\n",
    "            else:\n",
    "                all_utterances[curr_speaker].append(curr_utt)\n",
    "        \n",
    "        #assert(len(all_utterances['seeker'])==len(all_utterances['supporter']))\n",
    "\n",
    "        return all_utterances\n",
    "\n",
    "\n",
    "    def _prepare_data(self,split='train'):\n",
    "        data=self.splits[split]\n",
    "        \n",
    "        data_processed = defaultdict(dict)\n",
    "        inputs=[['<sos>']+tokens for tokens in data]\n",
    "        inputs=[tokens[:self.max_sequence_length] for tokens in inputs]\n",
    "\n",
    "        targets=[tokens[:self.max_sequence_length-1] for tokens in data]\n",
    "        targets=[tokens+['<eos>'] for tokens in targets]\n",
    "\n",
    "        lengths=[len(tokens) for tokens in inputs]\n",
    "\n",
    "        [inputs[i].extend(['<pad>'] * (self.max_sequence_length-lengths[i])) for i in range(len(inputs))]\n",
    "        [targets[i].extend(['<pad>'] * (self.max_sequence_length-lengths[i])) for i in range(len(targets))]\n",
    "\n",
    "        inputs=[[self.w2i.get(w,self.w2i['<unk>']) for w in tokens] for tokens in inputs]\n",
    "        targets=[[self.w2i.get(w,self.w2i['<unk>']) for w in tokens] for tokens in targets]\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            data_processed[i]['input']=inputs[i]\n",
    "            data_processed[i]['target']=targets[i]\n",
    "            data_processed[i]['length']=lengths[i]\n",
    "        \n",
    "        return data_processed\n",
    "    \n",
    "    def _create_vocab(self):\n",
    "        # create vocab from trainset\n",
    "        self.vocab=dict(sorted(self.cv.vocabulary_.items(),key=lambda x: x[1]))\n",
    "        # +4 to all indexs to make way for special tokens\n",
    "        self.vocab={k:v+4 for k,v in self.vocab.items()}\n",
    "\n",
    "        # add special tokens\n",
    "        self.special_tokens = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "        for i,token in enumerate(special_tokens):\n",
    "            self.vocab[token]=i\n",
    "\n",
    "        self.w2i=dict(sorted(self.vocab.items(),key=lambda x: x[1]))\n",
    "        self.i2w={i:w for w,i in w2i.items()}\n",
    "\n",
    "        # store w2i and i2w in dict\n",
    "        self.vocab=dict(w2i=w2i, i2w=i2w)\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.w2i)\n",
    "\n",
    "    @property\n",
    "    def pad_idx(self):\n",
    "        return self.w2i['<pad>']\n",
    "\n",
    "    @property\n",
    "    def sos_idx(self):\n",
    "        return self.w2i['<sos>']\n",
    "\n",
    "    @property\n",
    "    def eos_idx(self):\n",
    "        return self.w2i['<eos>']\n",
    "\n",
    "    @property\n",
    "    def unk_idx(self):\n",
    "        return self.w2i['<unk>']\n",
    "\n",
    "    def get_w2i(self):\n",
    "        return self.w2i\n",
    "\n",
    "    def get_i2w(self):\n",
    "        return self.i2w\n",
    "\n",
    "trainset=ESCONV(load_split='train')\n",
    "valset=ESCONV(load_split='valid')\n",
    "testset=ESCONV(load_split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=dict(zip(['train', 'valid','test'],[trainset,valset,testset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SentenceVAE\n",
    "\n",
    "params = dict(\n",
    "    vocab_size=datasets['train'].vocab_size,\n",
    "    sos_idx=datasets['train'].sos_idx,\n",
    "    eos_idx=datasets['train'].eos_idx,\n",
    "    pad_idx=datasets['train'].pad_idx,\n",
    "    unk_idx=datasets['train'].unk_idx,\n",
    "    max_sequence_length=50,\n",
    "    embedding_size=300,\n",
    "    rnn_type='gru',\n",
    "    hidden_size=256,\n",
    "    word_dropout=0,\n",
    "    embedding_dropout=.5,\n",
    "    latent_size=16,\n",
    "    num_layers=1,\n",
    "    bidirectional=True\n",
    ")\n",
    "\n",
    "model=SentenceVAE(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset=datasets['train'],\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "batch=next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-9.1588, -8.6968, -8.9289,  ..., -8.7510, -9.2895, -8.9929],\n",
       "          [-9.5271, -9.0473, -9.0330,  ..., -9.1774, -9.0350, -8.8178],\n",
       "          [-9.3848, -8.9214, -9.2440,  ..., -8.9341, -9.2758, -9.0847],\n",
       "          ...,\n",
       "          [-9.0099, -9.0418, -9.0322,  ..., -9.0084, -9.0237, -9.0088],\n",
       "          [-9.0099, -9.0418, -9.0322,  ..., -9.0084, -9.0237, -9.0088],\n",
       "          [-9.0099, -9.0418, -9.0322,  ..., -9.0084, -9.0237, -9.0088]],\n",
       " \n",
       "         [[-9.4589, -8.9679, -8.8549,  ..., -9.1017, -8.9892, -8.9292],\n",
       "          [-9.1719, -9.0426, -9.1878,  ..., -9.0510, -8.6918, -9.1229],\n",
       "          [-8.8628, -8.8871, -9.0365,  ..., -9.2286, -8.9645, -8.7401],\n",
       "          ...,\n",
       "          [-9.0099, -9.0418, -9.0322,  ..., -9.0084, -9.0237, -9.0088],\n",
       "          [-9.0099, -9.0418, -9.0322,  ..., -9.0084, -9.0237, -9.0088],\n",
       "          [-9.0099, -9.0418, -9.0322,  ..., -9.0084, -9.0237, -9.0088]],\n",
       " \n",
       "         [[-8.8912, -8.9500, -8.8911,  ..., -8.6639, -9.4993, -9.4273],\n",
       "          [-8.5864, -8.9749, -8.9967,  ..., -8.5192, -9.3535, -8.8873],\n",
       "          [-8.6000, -8.9235, -9.0549,  ..., -8.5254, -9.3388, -9.2612],\n",
       "          ...,\n",
       "          [-9.0099, -9.0418, -9.0322,  ..., -9.0084, -9.0237, -9.0088],\n",
       "          [-9.0099, -9.0418, -9.0322,  ..., -9.0084, -9.0237, -9.0088],\n",
       "          [-9.0099, -9.0418, -9.0322,  ..., -9.0084, -9.0237, -9.0088]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-8.7736, -8.5824, -9.2364,  ..., -9.0730, -9.5885, -8.7428],\n",
       "          [-8.9424, -8.3994, -9.0079,  ..., -9.0917, -9.4599, -8.8055],\n",
       "          [-8.6792, -8.9577, -9.1230,  ..., -8.9149, -8.9783, -8.8528],\n",
       "          ...,\n",
       "          [-9.0099, -9.0418, -9.0322,  ..., -9.0084, -9.0237, -9.0088],\n",
       "          [-9.0099, -9.0418, -9.0322,  ..., -9.0084, -9.0237, -9.0088],\n",
       "          [-9.0099, -9.0418, -9.0322,  ..., -9.0084, -9.0237, -9.0088]],\n",
       " \n",
       "         [[-8.9874, -8.8326, -8.6339,  ..., -8.9909, -9.0075, -9.0344],\n",
       "          [-8.9829, -8.7877, -8.7240,  ..., -9.2254, -8.9863, -8.9319],\n",
       "          [-8.8451, -8.8736, -8.8782,  ..., -8.8010, -8.9973, -9.2701],\n",
       "          ...,\n",
       "          [-9.0099, -9.0418, -9.0322,  ..., -9.0084, -9.0237, -9.0088],\n",
       "          [-9.0099, -9.0418, -9.0322,  ..., -9.0084, -9.0237, -9.0088],\n",
       "          [-9.0099, -9.0418, -9.0322,  ..., -9.0084, -9.0237, -9.0088]],\n",
       " \n",
       "         [[-9.0643, -8.8294, -8.9007,  ..., -8.8175, -9.3225, -9.2509],\n",
       "          [-9.1701, -8.9135, -9.4061,  ..., -8.7987, -9.0316, -9.2487],\n",
       "          [-8.9000, -8.7699, -9.2362,  ..., -9.0296, -9.1734, -9.1618],\n",
       "          ...,\n",
       "          [-9.0099, -9.0418, -9.0322,  ..., -9.0084, -9.0237, -9.0088],\n",
       "          [-9.0099, -9.0418, -9.0322,  ..., -9.0084, -9.0237, -9.0088],\n",
       "          [-9.0099, -9.0418, -9.0322,  ..., -9.0084, -9.0237, -9.0088]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([[ 2.0908e-01,  3.5623e-01,  1.4348e-02,  1.0526e-01, -4.1378e-03,\n",
       "          -4.3871e-01,  5.3494e-02,  6.7013e-02, -7.9696e-02,  8.6349e-03,\n",
       "           3.2229e-01, -1.3525e-02, -4.8585e-02, -1.8566e-01,  1.1945e-01,\n",
       "          -1.4766e-01],\n",
       "         [-9.2052e-03,  2.8155e-01, -2.8154e-01,  1.6035e-01, -2.2185e-01,\n",
       "          -5.0589e-03, -2.1009e-02,  2.3911e-02, -1.8216e-01, -1.7177e-01,\n",
       "           6.6179e-02, -8.4966e-02, -4.3672e-01, -2.5018e-01, -2.1899e-03,\n",
       "           6.3909e-02],\n",
       "         [ 5.2201e-02, -7.8022e-02, -2.1581e-01, -4.4231e-02, -1.5820e-03,\n",
       "          -1.5480e-01,  2.4184e-01,  2.0389e-01,  1.3200e-01,  1.1124e-01,\n",
       "           2.0810e-01, -6.0640e-02,  2.6108e-01, -3.4241e-02,  2.0570e-01,\n",
       "          -1.4218e-01],\n",
       "         [-5.5932e-02,  2.2672e-01, -2.1587e-01,  3.8869e-02, -1.0814e-01,\n",
       "          -2.5690e-01,  1.2579e-01,  9.3721e-02, -2.1989e-01, -2.3538e-02,\n",
       "           4.0161e-02,  2.9836e-02, -8.6029e-02, -2.3022e-01, -3.0698e-01,\n",
       "           1.5324e-01],\n",
       "         [-3.5715e-02,  7.3842e-02,  2.5532e-01,  1.6417e-02, -1.1106e-02,\n",
       "          -2.5774e-01,  1.3690e-01, -8.9555e-02, -2.1556e-02, -1.2126e-01,\n",
       "           1.0215e-01, -6.5724e-02,  2.0488e-01,  1.3670e-01,  1.2125e-01,\n",
       "           7.0318e-02],\n",
       "         [ 3.3660e-01,  2.9222e-01, -2.1291e-01,  3.8959e-01, -3.0253e-02,\n",
       "           2.3942e-01, -2.0183e-01, -3.4479e-01,  1.8748e-01,  5.5444e-02,\n",
       "           1.9982e-01, -2.5195e-03,  1.6264e-01, -1.1119e-01,  5.8416e-02,\n",
       "          -1.2129e-01],\n",
       "         [ 3.2547e-01,  1.5643e-02,  1.4680e-01, -1.1897e-01,  5.7744e-02,\n",
       "          -7.5448e-02,  5.2161e-02, -2.9204e-02,  2.2046e-01, -3.5733e-02,\n",
       "          -3.6458e-01,  1.1308e-01,  1.6687e-01, -1.5182e-01, -3.0353e-01,\n",
       "           5.9218e-02],\n",
       "         [-5.9856e-02,  5.1069e-01,  1.7785e-01, -1.2834e-01,  1.6645e-01,\n",
       "          -1.5051e-01,  4.6753e-01,  2.1185e-01,  7.8183e-02, -4.5380e-01,\n",
       "          -7.7602e-02, -2.9433e-01,  3.5039e-04, -2.2961e-02,  7.6349e-02,\n",
       "          -8.1121e-02],\n",
       "         [ 2.0608e-01, -3.4340e-02,  1.0590e-01, -3.6220e-02,  7.8468e-02,\n",
       "          -6.5595e-02, -2.1149e-01, -6.4877e-02, -2.0859e-02, -2.2265e-01,\n",
       "           1.3913e-02,  1.3177e-01, -7.2817e-02, -2.4423e-01, -2.1449e-01,\n",
       "           6.2951e-02],\n",
       "         [ 2.0077e-01, -1.4896e-01,  8.3606e-02, -6.6381e-02, -9.1906e-03,\n",
       "          -1.3008e-01, -9.3247e-02,  1.0665e-02, -1.6045e-02, -4.0428e-01,\n",
       "          -8.6195e-02, -3.7773e-03,  2.1332e-02, -2.4781e-01, -2.1348e-01,\n",
       "           4.6413e-02],\n",
       "         [ 2.7530e-01, -1.9812e-01, -1.0209e-01, -1.5452e-01,  1.5239e-01,\n",
       "           1.5067e-01, -3.6266e-01,  8.5598e-02, -1.7190e-01, -2.9197e-01,\n",
       "          -4.2807e-02, -4.3617e-02, -3.6490e-02, -3.0350e-01, -2.2702e-01,\n",
       "           3.0569e-03],\n",
       "         [ 1.5467e-01, -9.6032e-02,  1.0078e-01, -2.6905e-01,  1.1198e-01,\n",
       "           7.2319e-02, -3.9137e-01,  1.2654e-01,  2.9082e-02, -3.4577e-01,\n",
       "           3.0088e-02, -1.2312e-01,  1.3584e-01, -2.4170e-01, -1.6282e-01,\n",
       "          -5.1119e-02],\n",
       "         [ 1.3912e-01, -4.8268e-02,  1.1153e-01, -1.8194e-01,  1.4868e-01,\n",
       "           6.2036e-02, -1.5455e-01,  6.0439e-02,  6.0568e-02, -3.9285e-01,\n",
       "          -9.2866e-02, -6.8791e-02,  5.4657e-02, -4.9253e-02, -2.3873e-01,\n",
       "           1.2344e-01],\n",
       "         [ 1.9035e-01, -3.6072e-02,  7.1561e-02, -2.4011e-01,  2.1934e-01,\n",
       "          -1.1786e-01, -1.1967e-01, -1.1410e-01,  3.6936e-04, -3.2490e-01,\n",
       "          -1.3230e-01,  7.3453e-02,  2.3736e-01, -1.8923e-01, -2.6965e-01,\n",
       "           1.2489e-01],\n",
       "         [ 1.1906e-01, -2.7229e-01,  1.9284e-02, -8.7058e-02,  1.3165e-01,\n",
       "          -1.6022e-01, -3.1723e-02, -1.0426e-02,  1.3671e-02, -2.3257e-01,\n",
       "          -6.0282e-02, -7.8091e-02, -6.5924e-02, -5.2067e-02, -2.1232e-01,\n",
       "           1.9502e-01],\n",
       "         [ 1.5270e-01, -2.5001e-01,  6.1986e-02, -8.3312e-02, -5.6637e-03,\n",
       "           5.9283e-03,  4.6398e-02,  3.3196e-02,  6.5350e-02, -4.0922e-01,\n",
       "           7.9549e-02, -7.0533e-02, -1.3666e-02, -8.0870e-02, -4.4569e-01,\n",
       "           3.4618e-01]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[ 1.7980e-01, -1.0944e-01,  2.5866e-01,  1.0641e-01, -1.4059e-01,\n",
       "           1.4471e-01, -1.8013e-03, -4.9748e-02, -1.6960e-01,  2.1376e-01,\n",
       "           2.2170e-01,  1.3847e-01,  1.4455e-02,  2.2563e-02,  3.5550e-01,\n",
       "          -1.2524e-01],\n",
       "         [ 2.7691e-01,  1.2337e-01,  5.2053e-02, -9.9631e-02, -2.0649e-01,\n",
       "          -3.9745e-01,  1.8342e-01,  3.0517e-02, -1.7034e-01,  3.3397e-01,\n",
       "           4.2399e-02,  1.5382e-01, -1.0034e-01, -1.8305e-02,  8.5401e-02,\n",
       "          -6.2411e-02],\n",
       "         [ 3.2072e-01,  1.5838e-02,  1.9422e-01, -1.8788e-01,  2.0066e-01,\n",
       "          -1.3758e-01,  3.2068e-01, -2.1566e-01, -1.0161e-01,  4.1209e-02,\n",
       "           3.0279e-02, -1.0060e-01,  1.1191e-01, -1.1551e-01,  4.8071e-02,\n",
       "           4.8388e-02],\n",
       "         [ 2.0502e-01, -1.3684e-01,  1.2512e-01, -2.7638e-01, -2.8606e-01,\n",
       "           1.1232e-01,  2.4971e-01, -5.0984e-02, -1.8350e-01,  2.0489e-02,\n",
       "          -7.2326e-02, -1.4090e-02, -4.5938e-02,  2.0220e-01,  4.4758e-02,\n",
       "          -1.6657e-01],\n",
       "         [ 1.2494e-01, -6.8280e-02,  2.3741e-01, -1.7876e-01,  2.9850e-01,\n",
       "          -5.4270e-02, -3.6045e-01, -2.6704e-01,  7.1582e-02, -8.2721e-02,\n",
       "          -3.6550e-02,  8.4152e-02,  2.5656e-01, -2.4075e-01, -8.5797e-02,\n",
       "          -4.0483e-02],\n",
       "         [-1.6324e-01, -3.6255e-02,  2.2352e-01, -5.2855e-02,  1.9796e-01,\n",
       "          -1.5924e-01, -9.3394e-03, -6.1095e-02,  2.3728e-04,  1.4833e-01,\n",
       "           7.1678e-02,  6.2669e-02, -1.1611e-01,  6.4238e-02,  1.4452e-01,\n",
       "          -4.9698e-02],\n",
       "         [-1.0388e-01, -8.3115e-02,  1.8497e-01, -1.1259e-01,  1.8509e-01,\n",
       "          -5.8476e-02,  2.7538e-02, -1.3565e-01, -8.0655e-02,  1.1182e-01,\n",
       "          -6.0524e-02,  3.4180e-02,  2.5713e-01, -2.3190e-01, -1.1386e-01,\n",
       "           1.1107e-01],\n",
       "         [ 2.8453e-02,  1.0817e-02,  9.4974e-02, -7.5841e-02, -3.7041e-02,\n",
       "          -1.2307e-01, -4.0074e-02, -1.5904e-01, -2.6472e-01, -1.6349e-01,\n",
       "          -1.5982e-01, -1.6597e-01,  2.1855e-01,  1.1258e-01,  1.8474e-01,\n",
       "           3.1114e-01],\n",
       "         [-3.4841e-01,  2.9217e-01, -1.1336e-01,  1.7859e-01,  1.9622e-01,\n",
       "          -2.2324e-01,  7.1808e-02, -1.3312e-01,  1.7587e-01,  1.5452e-02,\n",
       "           2.2754e-01, -1.5229e-01,  1.1240e-01,  7.3746e-02, -2.4496e-02,\n",
       "          -5.9808e-02],\n",
       "         [-4.4545e-01,  2.5559e-01, -2.2451e-01,  1.5523e-01,  1.7724e-01,\n",
       "          -2.1425e-01,  2.1664e-02, -2.0846e-01,  2.1010e-02, -2.0571e-01,\n",
       "           3.1664e-01, -7.4013e-02,  1.9751e-01,  1.7923e-01, -9.2402e-02,\n",
       "          -8.4115e-02],\n",
       "         [-3.7294e-01,  1.9393e-02, -1.7271e-02,  3.7438e-01,  4.0171e-01,\n",
       "          -2.2229e-02, -9.3465e-02, -7.1327e-02,  1.0633e-01, -1.5669e-01,\n",
       "           1.5081e-01, -4.8843e-02,  2.7971e-01,  1.3263e-01, -1.2222e-01,\n",
       "          -1.3678e-01],\n",
       "         [-1.8732e-01,  2.8480e-01, -1.2360e-01,  2.4376e-01,  3.0496e-01,\n",
       "          -1.5604e-01, -1.9136e-01, -2.4379e-01,  4.7529e-02, -1.6540e-01,\n",
       "           1.2086e-01, -2.8148e-01,  1.2091e-01,  2.2904e-01, -3.6854e-02,\n",
       "          -1.5412e-01],\n",
       "         [-2.9579e-01,  1.6000e-01, -2.1487e-01,  3.0617e-01,  2.9169e-01,\n",
       "          -1.4854e-02, -7.5396e-02, -1.2311e-01,  1.2634e-01, -5.3045e-02,\n",
       "           2.6321e-01, -2.1561e-01,  5.2272e-02,  1.6672e-01, -1.2396e-01,\n",
       "           4.0996e-02],\n",
       "         [-3.4094e-01,  1.3226e-01, -2.0945e-01,  3.1452e-01,  2.1310e-01,\n",
       "          -1.9158e-01,  1.0623e-04, -8.8280e-02,  3.0662e-02,  6.3548e-02,\n",
       "           2.8435e-01, -1.0676e-01,  1.8543e-01,  2.7563e-01, -2.7087e-01,\n",
       "          -8.5569e-02],\n",
       "         [-2.2428e-01,  8.4144e-02,  1.5131e-02,  3.3789e-01,  1.6014e-01,\n",
       "          -8.3367e-02, -3.1758e-02, -1.8672e-02,  1.6275e-02, -1.2367e-01,\n",
       "           2.3436e-01, -2.6400e-01,  2.6248e-01,  2.1115e-01, -1.9361e-01,\n",
       "          -7.6230e-02],\n",
       "         [-3.3254e-01,  2.1784e-01, -1.5542e-01,  2.6057e-01,  3.0531e-01,\n",
       "          -1.0502e-01, -8.4577e-02, -2.3505e-01,  3.4889e-02, -8.0995e-02,\n",
       "           3.9168e-01, -2.6854e-01,  1.3457e-01,  5.6823e-02, -2.0762e-01,\n",
       "           3.7886e-02]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[-9.2268e-01,  9.1835e-01, -1.4634e+00, -5.9014e-01, -1.1176e+00,\n",
       "          -1.3122e+00,  1.1224e+00,  1.4443e+00,  5.7846e-01, -2.1072e-01,\n",
       "          -9.1479e-02,  6.2792e-01, -2.4711e+00,  2.0807e+00,  1.3893e+00,\n",
       "          -6.9611e-01],\n",
       "         [-1.0196e+00,  7.1502e-01,  1.1379e+00,  9.5230e-01, -5.6987e-01,\n",
       "          -1.0388e-01, -1.2414e+00,  9.9489e-01,  4.0900e-01,  9.0113e-01,\n",
       "           2.1882e-01, -4.3703e-01, -4.4028e-01, -6.3096e-01, -4.7045e-01,\n",
       "           5.8340e-01],\n",
       "         [ 5.4661e-01, -1.1905e-01, -1.9418e-02, -1.0453e-01, -1.5273e+00,\n",
       "           7.5573e-01,  1.7217e+00, -3.9253e-01,  8.4484e-02,  1.3264e+00,\n",
       "           1.0193e+00, -7.5873e-01, -1.2426e+00, -4.8611e-01, -5.8879e-01,\n",
       "           5.5799e-01],\n",
       "         [ 8.1590e-01, -2.6393e-01,  6.9550e-01,  5.9065e-01, -4.7680e-02,\n",
       "          -6.1534e-01,  7.9591e-01,  5.9059e-01,  5.0570e-01,  1.2862e+00,\n",
       "           2.3799e-01, -5.8018e-02,  3.4862e-01, -1.3298e-01, -6.5565e-01,\n",
       "           1.6552e-01],\n",
       "         [-8.6890e-01,  1.1670e+00, -7.7155e-01, -2.4659e+00,  2.1985e+00,\n",
       "           4.3456e-01,  1.4323e-01, -1.0920e+00,  2.0442e+00,  1.5364e-01,\n",
       "          -2.6813e-01, -4.5359e-01,  8.0723e-01,  1.3900e-01,  1.2135e+00,\n",
       "           1.0179e-01],\n",
       "         [ 1.4609e+00, -1.6014e+00,  1.0322e+00,  3.3852e-01,  4.9685e-01,\n",
       "           5.6278e-01, -1.5356e+00, -1.2991e+00,  1.1587e+00, -6.2932e-02,\n",
       "           7.0286e-01, -3.2544e-01,  1.7495e+00, -1.3209e+00, -1.0220e+00,\n",
       "           4.1714e-01],\n",
       "         [ 2.2329e-01, -1.0981e-01, -2.9846e-01, -5.7526e-01,  1.5483e+00,\n",
       "           9.3691e-01,  4.5344e-01, -1.9107e-01, -5.7840e-02,  5.7558e-01,\n",
       "           5.6162e-01,  8.8770e-01,  2.4542e-01, -8.6121e-01, -5.3537e-01,\n",
       "           1.4641e+00],\n",
       "         [-2.1386e+00,  8.7492e-01, -1.2892e+00,  6.1083e-02,  5.4895e-01,\n",
       "           1.1762e+00, -1.6222e+00,  5.2990e-01,  1.0011e-01, -6.1192e-01,\n",
       "          -2.4831e-01, -2.3976e-02,  1.5085e+00, -3.3379e-01,  4.7630e-01,\n",
       "           3.9702e-01],\n",
       "         [-6.1449e-01,  1.8744e+00, -2.3912e-01, -9.5452e-01, -5.6592e-01,\n",
       "          -8.9907e-01, -7.3011e-01, -1.7214e+00,  1.1155e+00,  5.6387e-01,\n",
       "          -2.4397e-01,  1.4727e+00,  7.6697e-01,  5.0797e-01,  3.2744e-01,\n",
       "           6.4374e-02],\n",
       "         [-1.2036e+00, -7.8892e-01,  9.0272e-01, -2.7112e-01, -3.0143e-01,\n",
       "          -4.1527e-01, -4.0849e-01,  2.8835e-01, -4.5024e-01, -9.4741e-01,\n",
       "           1.2892e+00, -9.1825e-02,  1.5900e+00, -1.3683e+00, -9.4310e-02,\n",
       "          -6.8746e-01],\n",
       "         [ 1.2114e+00, -1.1976e-01,  5.0920e-03,  3.4030e-01, -1.5532e+00,\n",
       "           7.1330e-01, -1.5426e-01,  6.9990e-02,  6.4620e-01,  6.6245e-01,\n",
       "          -2.3791e-01,  9.9683e-01,  8.0130e-01, -2.6391e+00,  4.9411e-01,\n",
       "           6.9464e-01],\n",
       "         [ 5.1332e-01, -3.6168e-01, -3.4966e-01, -1.5886e+00,  1.8733e+00,\n",
       "          -3.9305e-02,  5.7001e-01, -8.7599e-01, -4.8965e-01, -3.2696e-01,\n",
       "          -6.6407e-01, -1.9845e-02, -1.4323e+00,  2.8447e-01,  1.3846e+00,\n",
       "           3.9438e-01],\n",
       "         [ 1.1653e+00, -1.5597e-01,  1.7881e-01,  2.9601e-02,  1.1693e+00,\n",
       "           2.5632e-01,  3.9500e-01,  2.1214e+00, -1.7196e+00,  5.0181e-01,\n",
       "           6.4825e-02, -7.9211e-01,  1.5899e+00,  4.6966e-01, -8.0525e-01,\n",
       "           9.9872e-01],\n",
       "         [ 1.0855e-03, -1.1625e+00,  3.4818e-01, -2.3964e-01, -2.2977e-01,\n",
       "           4.8728e-01,  6.6760e-02,  1.2781e+00, -3.0524e-01, -1.6279e+00,\n",
       "           1.2669e+00,  4.0841e-01,  1.0469e+00, -1.0883e+00,  2.0430e-01,\n",
       "          -1.8892e+00],\n",
       "         [ 2.0930e+00, -7.8490e-01,  6.0417e-01, -4.8004e-01,  7.1205e-02,\n",
       "           1.7210e-01,  3.5212e-01,  9.1375e-01, -1.2855e+00,  2.6110e-03,\n",
       "          -6.7664e-01, -8.1843e-01, -6.2650e-01, -9.3000e-01,  2.8866e-01,\n",
       "           1.0950e+00],\n",
       "         [-4.1281e-01,  8.5691e-01, -5.9352e-01, -1.2169e+00, -3.2838e+00,\n",
       "          -2.2608e-01,  5.4763e-01, -2.1802e-01,  5.9243e-01, -7.5683e-01,\n",
       "           3.2288e+00, -1.2502e+00, -2.5521e-01, -1.2499e+00, -1.6445e+00,\n",
       "          -1.1959e+00]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "model(batch['input'], batch['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion-vectors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ae7361b3d7bda8e25ee9741f2cfe2df01dd9a588b96d09c0b68846bde4c37e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
